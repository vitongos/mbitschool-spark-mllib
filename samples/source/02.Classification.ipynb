{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark MLlib\n",
    "\n",
    "### Ejercicio de Clasificación\n",
    "\n",
    "## Problema\n",
    "\n",
    "Ahora que ya hemos visto la API de MLlib en funcionamiento vamos a probarla sobre un problema de clasificación supervisada.\n",
    "\n",
    "En este caso estaremos tratando un problema estadístico muy conocido llamado **Adult**. Este dataset plantea estimar el salario medio de un censo poblacional en función de una serie de características.\n",
    "\n",
    "La descripción original es la que sigue:\n",
    "\n",
    ">>>The Adult dataset we are going to use is publicly available at the UCI Machine Learning Repository. This data derives from census data, and consists of information about 48842 individuals and their annual income. We will use this information to predict if an individual earns >50k a year or <=50K a year. The dataset is rather clean, and consists of both numeric and categorical variables.\n",
    "\n",
    "**Attribute Information:**\n",
    "- age: continuous\n",
    "- workclass: Private,Self-emp-not-inc, Self-emp-inc, Federal-gov, Local-gov, State-gov, Without-pay, Never-worked\n",
    "- fnlwgt: continuous\n",
    "- education: Bachelors, Some-college, 11th, HS-grad, Prof-school, Assoc-acdm, Assoc-voc...\n",
    "- education-num: continuous\n",
    "- marital-status: Married-civ-spouse, Divorced, Never-married, Separated, Widowed, Married-spouse-absent...\n",
    "- occupation: Tech-support, Craft-repair, Other-service, Sales, Exec-managerial, Prof-specialty, Handlers-cleaners...\n",
    "- relationship: Wife, Own-child, Husband, Not-in-family, Other-relative, Unmarried\n",
    "- race: White, Asian-Pac-Islander, Amer-Indian-Eskimo, Other, Black\n",
    "- sex: Female, Male\n",
    "- capital-gain: continuous\n",
    "- capital-loss: continuous\n",
    "- hours-per-week: continuous\n",
    "- native-country: United-States, Cambodia, England, Puerto-Rico, Canada, Germany...\n",
    "- Target/Label: - <=50K, >50K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tareas a realizar:\n",
    "\n",
    "- Indexar las categorías. Transformar las variables categóricas a numéricas utilizando el transformador `StringIndexer`\n",
    "- Vectorizar la lista de categorías\n",
    "- Aprender mediante validación cruzada dos algoritmos (`LogisticRegresion` y `DecisionTree`)\n",
    "  - Utilizar para ello el evaluador `BinaryClassificationEvaluator``\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cargando los datos:\n",
    "\n",
    "Vamos a cargar el dataset utilizando la función `read.csv`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls /opt/spark-data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfAdult = spark.read.csv(\"/opt/spark-data/adult.data\", inferSchema=True)\n",
    "\n",
    "# Vamos a asignar a todas las variables su nombre correcto\n",
    "dfAdult = dfAdult.toDF(\"age\", \"workclass\", \"fnlwgt\", \"education\", \"education_num\", \"marital_status\", \"occupation\", \"relationship\", \"race\", \"sex\", \"capital_gain\", \"capital_loss\", \"hours_per_week\", \"native_country\" , \"income\")\n",
    "\n",
    "cols = dfAdult.columns\n",
    "dfAdult.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfAdult.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import OneHotEncoderEstimator, StringIndexer, VectorAssembler\n",
    "\n",
    "categoricalColumns = [\"workclass\", \"education\", \"marital_status\", \"occupation\", \"relationship\", \"race\", \"sex\", \"native_country\"]\n",
    "stages = [] # stages del Pipeline\n",
    "\n",
    "# Pasamos a índices las columnas categóricas con StringIndexer\n",
    "\n",
    "for categoricalCol in categoricalColumns:\n",
    "    stringIndexer = StringIndexer(inputCol=categoricalCol, outputCol=categoricalCol + \"Index\")\n",
    "    # Use OneHotEncoder to convert categorical variables into binary SparseVectors\n",
    "    # encoder = OneHotEncoderEstimator(inputCol=categoricalCol + \"Index\", outputCol=categoricalCol + \"classVec\")\n",
    "    encoder = OneHotEncoderEstimator(inputCols=[stringIndexer.getOutputCol()], outputCols=[categoricalCol + \"classVec\"])\n",
    "    stages += [stringIndexer, encoder]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertimos la columna de clase en 'label' con StringIndexer\n",
    "\n",
    "label_stringIdx = StringIndexer(inputCol=\"income\", outputCol=\"label\")\n",
    "stages += [label_stringIdx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformamos las features en un Vector mediante VectorAssembler\n",
    "\n",
    "numericCols = [\"age\", \"fnlwgt\", \"education_num\", \"capital_gain\", \"capital_loss\", \"hours_per_week\"]\n",
    "\n",
    "assemblerInputs = [c + \"Index\" for c in categoricalColumns] + numericCols\n",
    "\n",
    "assembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\n",
    "stages += [assembler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "  \n",
    "partialPipeline = Pipeline().setStages(stages)\n",
    "pipelineModel = partialPipeline.fit(dfAdult)\n",
    "preppedDataDF = pipelineModel.transform(dfAdult)\n",
    "\n",
    "# Enrenar el modelo para los datos preparados\n",
    "\n",
    "lrModel = LogisticRegression().fit(preppedDataDF)\n",
    "\n",
    "# ROC de los datos de entrenamiento\n",
    "\n",
    "lrModel.summary.roc.show()\n",
    "\n",
    "display(lrModel, preppedDataDF)\n",
    "\n",
    "# Keep relevant columns\n",
    "\n",
    "selectedcols = [\"label\", \"features\"] + cols\n",
    "dataset = preppedDataDF.select(selectedcols)\n",
    "display(dataset)\n",
    "\n",
    "# Dividimos el dataset en 70% para training y 30% para testing.\n",
    "\n",
    "dfTrain, dfTest = dataset.randomSplit([0.7, 0.3], seed=1234)\n",
    "print(\"Tenemos %d filas de training y %d filas de test.\" % (dfTrain.count(), dfTest.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Create initial LogisticRegression model\n",
    "lr = LogisticRegression(labelCol=\"label\", featuresCol=\"features\", maxIter=10)\n",
    "\n",
    "# Train model with Training Data\n",
    "lrModel = lr.fit(dfTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on test data using the transform() method.\n",
    "# LogisticRegression.transform() will only use the 'features' column.\n",
    "\n",
    "predictions = lrModel.transform(dfTest)\n",
    "\n",
    "# View model's predictions and probabilities of each prediction class\n",
    "# You can select any columns in the above schema to view as well.\n",
    "# For example's sake we will choose age & occupation\n",
    "\n",
    "selected = predictions.select(\"label\", \"prediction\", \"probability\", \"age\", \"occupation\")\n",
    "selected.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# Evaluación del modelo\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lr.explainParams())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees\n",
    "\n",
    "Vamos a utilizar otro algoritmo de clasificación: árboles de decisión.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "\n",
    "# Crear el modelo\n",
    "\n",
    "dt = DecisionTreeClassifier(labelCol=\"label\", featuresCol=\"features\", maxDepth=20, maxBins=500)\n",
    "\n",
    "# Entrenar el modelo\n",
    "\n",
    "dtModel = dt.fit(dfTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Podemos explorar el número de nodos del árbol o su profundidad\n",
    "\n",
    "print(\"numNodes = \", dtModel.numNodes)\n",
    "print(\"depth = \", dtModel.depth)\n",
    "\n",
    "display(dtModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hacer las predicciones en los datos de tests con Transformer.transform().\n",
    "\n",
    "predictions = dtModel.transform(dfTest)\n",
    "predictions.printSchema()\n",
    "\n",
    "# Examinamos las predicciones y probabilidades para cada clase\n",
    "\n",
    "selected = predictions.select(\"label\", \"prediction\", \"probability\", \"age\", \"occupation\")\n",
    "selected.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# Evaluamos la precisión del árbol de decisión\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
