{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducción"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operaciones estadísticas: Correlación\n",
    "\n",
    "Podemos computar la correlación entre las columnas de nuestro dataframe.\n",
    "\n",
    "Primero generaremos un dataframe de pruebas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[1.0, 0.0, 0.0, -2.0],\n",
    "        [2.0, 5.0, 0.0, 3.0],\n",
    "        [3.0, 7.0, 0.0, 8.0],\n",
    "        [4.0, 0.0, 0.0, 1.0],\n",
    "       ]\n",
    "\n",
    "df = spark.createDataFrame(data) \n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este dataframe no es válido para poderse utilizar con MLlib, ya que no está vectorizado.\n",
    "\n",
    "Para vectorizarlo podemos utilizar transformadores que veremos más adelante.\n",
    "\n",
    "Lo definiremos en modo vectorizado usando vectores sparse y dense conforme corresponda.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "data = [(Vectors.sparse(4, [(0, 1.0), (3, -2.0)]),),\n",
    "        (Vectors.dense([4.0, 5.0, 0.0, 3.0]),),\n",
    "        (Vectors.dense([6.0, 7.0, 0.0, 8.0]),),\n",
    "        (Vectors.sparse(4, [(0, 9.0), (3, 1.0)]),)]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"features\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora podemos lanzar el método Correlation de MLlib sin problemas, especificando la columna del dataframe donde están los vectores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.stat import Correlation\n",
    "\n",
    "r1 = Correlation.corr(df, \"features\").head()\n",
    "\n",
    "print(\"Pearson correlation matrix:\\n\" + str(r1[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a testear el mismo método con un dataframe algo más potente, generado aleatoriamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "a = [ random.random() for x in range(1, 1000)]\n",
    "b = [ x * 2 for x in a ]\n",
    "\n",
    "dataList = [ [a[i], b[i]] for i in range(0, len(a)) ]\n",
    "dataML = [ (Vectors.dense([a[i], b[i]]),) for i in range(0, len(a))]\n",
    "\n",
    "dfList = spark.createDataFrame(dataList, [\"features\"])\n",
    "dfVectors = spark.createDataFrame(dataML, [\"features\"])\n",
    "\n",
    "display(dataList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r1 = Correlation.corr(dfVectors, \"features\").head()\n",
    "\n",
    "print(\"Pearson correlation matrix:\\n\" + str(r1[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regresión Lineal\n",
    "\n",
    "Podemos encontrar varios problemas de regresión en los ejemplos que propone databricks.\n",
    "\n",
    "Utilizaremos un problema que plantea la estimación de éxito de un servicio de alquiler de bicicletas.\n",
    "Esta es la descripción oficial que nos ofrece el repositorio de datos UCI\n",
    "\n",
    "#### Data description\n",
    "\n",
    "**Feature columns:**\n",
    "- dteday: date\n",
    "- season: season (1:spring, 2:summer, 3:fall, 4:winter)\n",
    "- yr: year (0:2011, 1:2012)\n",
    "- mnth: month (1 to 12)\n",
    "- hr: hour (0 to 23)\n",
    "- holiday: whether day is holiday or not\n",
    "- weekday: day of the week\n",
    "- workingday: if day is neither weekend nor holiday is 1, otherwise is 0.\n",
    "- weathersit:\n",
    "  - 1: Clear, Few clouds, Partly cloudy, Partly cloudy\n",
    "  - 2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist\n",
    "  - 3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds\n",
    "  - 4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog\n",
    "- temp: Normalized temperature in Celsius. The values are derived via `(t-t_min)/(t_max-t_min)`, `t_min=-8`, `t_max=+39` (only in hourly scale)\n",
    "- atemp: Normalized feeling temperature in Celsius. The values are derived via `(t-t_min)/(t_max-t_min)`, `t_min=-16`, `t_max=+50` (only in hourly scale)\n",
    "- hum: Normalized humidity. The values are divided to 100 (max)\n",
    "- windspeed: Normalized wind speed. The values are divided to 67 (max)\n",
    "\n",
    "**Label columns:**\n",
    "- casual: count of casual users\n",
    "- registered: count of registered users\n",
    "- cnt: count of total rental bikes including both casual and registered\n",
    "\n",
    "**Extraneous columns:**\n",
    "- instant: record index\n",
    "\n",
    "For example, the first row is a record of hour 0 on January 1, 2011---and apparently 16 people rented bikes around midnight!\",\n",
    "\t\t\"subtype\": \"command\",\n",
    "\t\t\"position\": 13.0,\n",
    "\t\t\"command\": \"%md\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"/opt/spark-data/bike-sharing.csv\", header=True)\n",
    "\n",
    "df.cache()\n",
    "\n",
    "df.show(10)\n",
    "\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocesamiento de datos\n",
    "\n",
    "Aunque es un dataset preparado para ML y viene bastante limpio, debemos realizar algunas modificaciones antes de aprender un modelo.\n",
    "\n",
    "Nuestro objetivo es predecir la variable `cnt`. ¿Podemos utilizar todas las variables del problema?\n",
    "\n",
    "- Problema 1: La variable `cnt` se calcula como la suma `casual + registered`. Estas dos variables, al igual que `cnt`, nunca estarán disponibles cuando recibamos nuevos datos, así que es fundamental no incluirlas en nuestros modelos. De hacerlo estaríamos incurriendo en lo que se conoce como una *Fuga de datos* o *data leak*.\n",
    "\n",
    "- Problema 2: La variable `dteday` es incómoda de representar para los algoritmos (que tipo de distribución tiene), y ha sido previamente separada en categorías más sencillas como el mes, la hora y el día de la semana. Es importante destacar que estas variables ya vienen en formato numérico o binario para que los algoritmos las traten correctamente, algo que nos agiliza enormemente el trabajo.\n",
    "\n",
    "- Problema 3: El identificador `instant` no tiene ningún valor para nuestro problema y podría suponer un problema para algunos modelos incluso.\n",
    "\n",
    "Con este pequeño análisis concluimos que es necesario eliminar estas columnas de nuestro dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfClean = df.drop(\"instant\").drop(\"dteday\").drop(\"casual\").drop(\"registered\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como ya hemos comentado, las variables has sido correctamente preprocesadas en un formato compatible con nuestros algoritmos de ML. No obstante, al haberlo leído desde un fichero CSV *que no almacena el tipo* de las variables es probable que dichos tipos se hayan perdido.\n",
    "\n",
    "Vamos a comprobarlo leyendo el esquema de los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfClean.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El resultado que podemos observar es que todas las variables son de tipo string, por ello es importante que las convirtamos a tipo numérico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "dfCasted = df.select([ col(c).cast(\"double\") for c in dfClean.columns ])\n",
    "\n",
    "dfCasted.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtener una muestra de training y test\n",
    "\n",
    "Antes de aprender el modelo debemos reservar algunos datos para su evaluación, asegurando así que el test que realicemos se hará sobre datos que el modelo nunca ha podido observar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividimos el dataset en 70% para training y 30% para testing.\n",
    "\n",
    "dfTrain, dfTest = dfCasted.randomSplit([0.7, 0.3], seed=1234)\n",
    "print(\"Tenemos %d filas de training y %d filas de test.\" % (dfTrain.count(), dfTest.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualización\n",
    "\n",
    "Ahora que tenemos los datos preparados, vamos a observar el dataset para determinar la potencia discriminativa de las variables que tenemos a nuestra disposición y hacernos una idea de cómo debería funcionar nuestro algoritmo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTrain.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento de un modelo de regresión\n",
    "\n",
    "Antes de poder lanzar el algoritmo de aprendizaje en sí, hemos de convertir el dataframe en un set de datos compatible como MLlib. Para ello debemos reducir nuestros datos a dos columnas (`features` y `label`).\n",
    "\n",
    "Para ello debemos definir y utilizar algunos de los **transformadores** que pone a nuestra disposición la librería.\n",
    "\n",
    "- `VectorAssembler`: Combina una serie de columnas en una única columna representada por vectores\n",
    "- `VectorIndexer`: Identifica las columnas de un vector que deben ser tratadas como categorías o como variables contínuas. Para ello utiliza un algoritmo heurístico que explora los valores de la columna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# We select the appropiate columns as features\n",
    "\n",
    "featuresCols = dfTrain.columns\n",
    "featuresCols.remove('cnt')\n",
    "\n",
    "# This concatenates all feature columns into a single feature vector in a new column \"rawFeatures\".\n",
    "\n",
    "vectorAssembler = VectorAssembler(inputCols=featuresCols, outputCol=\"rawFeatures\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los transformadores y estimadores tienen una sintaxis muy similar, para que podamos conectarlos unos con otros.\n",
    "\n",
    "- `inputCol`/`inputCols` La columna o columnas que recibe la función, debe existir en el dataframe.\n",
    "- `outputCol` La columna donde la función depositará su resultado en el dataframe resultante. Es importante poner nombres representativos para no cometer errores al encadenar métodos.\n",
    "- Parámetros/Hyperparámetros: En función de si la función es del tipo transformador o estimador tendremos uno o ambos conjuntos de opciones para parametrizar el proceso de aprendizaje o de transformación de datos. Este es un concepto fundamental en Machine Learning ya que condicionará lo bien que funcionen nuestros algoritmos.\n",
    "\n",
    "Además de estos conceptos básicos, las funciones de transformación tienen otra serie de métodos muy útiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shows its actual parametrization\n",
    "\n",
    "vectorAssembler.explainParams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns the name of input and output columns, useful for chaining methods programatically\n",
    "\n",
    "print( vectorAssembler.getInputCols() )\n",
    "print( vectorAssembler.getOutputCol() )\n",
    "\n",
    "# La función `transform` es la más importante de todas\n",
    "# Transformará nuestro dataframe, añadiendo la columna requerida.\n",
    "\n",
    "dfTrainAssembled = vectorAssembler.transform(dfTrain)\n",
    "dfTrainAssembled.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación realizaremos la indexación de las variables categóricas. Para ello crearemos un estimador del tipo `VectorIndexer`.\n",
    "\n",
    "**¿Por qué esta función es del tipo estimator?**\n",
    "\n",
    "`VectorIndexer` necesita procesar nuestros datos previamente a poder transformarlos, para determinar qué atributos son del tipo categórico. Para ello ha de recopilar estadísticos y almacenar esta información en un **modelo**. Éste modelo que será del tipo `VectorIndexerModel` tendrá el método transform que nos dejará procesar nuestros datos.\n",
    "\n",
    "Para entrenar un estimador se utiliza el método `fit` que recibe un dataframe de entrada.\n",
    "\n",
    "**Importante!!** Solo se debe llamar a fit con los datos de *entrenamiento*, despues se podrán transformar con el mismo modelo los datos de entrenamiento y test, de lo contrario estaríamos introduciendo información del test en el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorIndexer\n",
    "\n",
    "# This identifies categorical features and indexes them.\n",
    "\n",
    "vectorIndexer = VectorIndexer(inputCol=\"rawFeatures\", outputCol=\"features\", maxCategories=4)\n",
    "\n",
    "vectorIndexerModel = vectorIndexer.fit(dfTrainAssembled)\n",
    "\n",
    "dfTrainIndexed = vectorIndexerModel.transform(dfTrainAssembled)\n",
    "\n",
    "dfTrainIndexed.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora que hemos transformado nuestros datos al formato correcto, vamos a proseguir con el entrenamiento del modelo.\n",
    "\n",
    "Comenzaremos con uno de los algoritmos más sencillos, la **regresión lineal**.\n",
    "\n",
    "Este algoritmo está disponible bajo el paquete `LinearRegression`. Como ya podéis imaginar, esta clase es un **estimador** que debemos **entrenar** para poder obtener nuestro modelo. Para ello debemos proporcionarle los datos de entrenamiento e indicar las columnas que queremos utilizar como ´features´y como ´label´.\n",
    "\n",
    "Además de especificar los datos de entrada también podremos configurar el algoritmo de aprendizaje ajustando los distintos hyperparámetros, lo que tendrá un impacto importante en el resultado que obtengamos. Para el caso de la regresión tenemos un gran número de parámetros como por ejemplo:\n",
    "\n",
    "- `maxIter` Número de iteraciones a realizar\n",
    "- `regParam` Parámetro que indica el tipo de regularización (ninguna, L1, L2...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "lr = LinearRegression(labelCol=\"cnt\", featuresCol=\"features\", maxIter=20, regParam=0.0)\n",
    "\n",
    "lr.explainParam('maxIter')\n",
    "\n",
    "# Una vez creado el algoritmo de aprendizaje podemos entrenar un modelo pasándole los datos de entrada.\n",
    "\n",
    "lrModel = lr.fit(dfTrainIndexed)\n",
    "\n",
    "# Ya hemos aprendido un modelo de Machine Learning con el que podemos realizar predicciones.\n",
    "# En muchos casos es interesante echarle un vistazo a los parámetros del modelo. \n",
    "\n",
    "# En el caso de una regresión lineal los parámetros a consultar serían los coeficientes \n",
    "# y el término independiente (intercept).\n",
    "\n",
    "print(\"Coefficients: %s\" % (str(lrModel.coefficients.toArray())))\n",
    "print(\"Intercept: %f\" % (lrModel.intercept))\n",
    "\n",
    "# Summarize the model over the training set and print out some metrics\n",
    "trainingSummary = lrModel.summary\n",
    "print(\"numIterations: %d\" % trainingSummary.totalIterations)\n",
    "print(\"objectiveHistory: %s\" % str(trainingSummary.objectiveHistory))\n",
    "trainingSummary.residuals.show()\n",
    "print(\"RMSE: %f\" % trainingSummary.rootMeanSquaredError)\n",
    "print(\"r2: %f\" % trainingSummary.r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos utilizar nuestro modelo para obtener predicciones sobre nuestras instancias en una nueva columna. De este modo podemos analizar el resultado obtenido mediante métricas de evaluación y visualización."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTrainPredicctions = lrModel.transform(dfTrainIndexed)\n",
    "\n",
    "dfTrainPredicctions.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aunque la visualización puede darnos una buena muestra de cómo ha funcionado nuestro algoritmo, en la mayoría de ocasiones necesitamos poder obtener una métrica con interpretación estadística, que nos permita comparar cuantitativamente el resultado. \n",
    "\n",
    "En el caso de la regresión lineal lo más habitual es medir el **error cuadrático medio** o **RMSE**. Para ello spark nos proporciona una función que lo computa de manera escalable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"cnt\", predictionCol=\"prediction\")\n",
    "\n",
    "evaluator.evaluate(dfTrainPredicctions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este valor por si solo no tiene mucha importancia, necesitamos otros datos para poder compararlo. De momento, aplicaremos el resultado también a los datos de test. Para ello debemos transformar los datos usando todo el proceso anterior.\n",
    "\n",
    "**Muy importante**: Notad que solo estoy aplicando los métodos de transformación y en ningún momento volviendo a entrenar ningún modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTestAssembled = vectorAssembler.transform(dfTest)\n",
    "\n",
    "dfTestIndexed = vectorIndexerModel.transform(dfTestAssembled)\n",
    "\n",
    "dfTestPredictions = lrModel.transform(dfTestIndexed)\n",
    "\n",
    "evaluator.evaluate(dfTestPredictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El resultado es ligeramente peor, no obstante parece que el modelo ha **generalizado** correctamente y no se ha **sobreajustado** a los datos de entrenamiento. Para comprender un poco mejor este error en su contexto podemos contrastar el error con la distribución de la variable objetivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTrain.select(col(\"cnt\")).summary().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstracción del proceso mediante pipelines\n",
    "\n",
    "¿Cuál sería el siguiente paso para tratar de mejorar este algoritmo?\n",
    "\n",
    "Tenemos dos opciones:\n",
    "\n",
    "- Reentrenar el algoritmo con otros hyperparametros\n",
    "- Probar con otro método\n",
    "\n",
    "En ambos casos acabamos de ver que el proceso está lejos de ser reproducible y será complicado llevarlo a producción.\n",
    "\n",
    "Por ello Spark MLlib pone a nuestra disposición las pipelines (muy inspiradas en sklearn), para automatizar y exportar todo el proceso de entrenamiento y evaluación de modelos.\n",
    "\n",
    "Básicamente una pipeline es una secuencia de transformadores y estimadores que abstrae todo el proceso como un único estimador, sobre el que únicamente hay que realizar las operaciones de `fit`y `transform` una sola vez."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "pipelineLr = Pipeline(stages=[vectorAssembler, vectorIndexer, lr])\n",
    "\n",
    "# Usar el dataset de entrenamiento original\n",
    "\n",
    "pipelineModel = pipelineLr.fit(dfTrain)\n",
    "\n",
    "dfTestPredictions = pipelineModel.transform(dfTest) \n",
    "\n",
    "dfTestPredictions.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez creada la pipeline es muy sencillo reusarla para entrenar y evaluar otros modelos.\n",
    "\n",
    "A continuación vamos a resolver este mismo problema con otro modelo distinto, mucho más potente, como es la regresión mediante `GBT` Gradient boosted trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import GBTRegressor\n",
    "\n",
    "gbt = GBTRegressor(labelCol=\"cnt\", maxDepth=3, maxIter=20)\n",
    "\n",
    "pipelineGbt = Pipeline(stages=[vectorAssembler, vectorIndexer, gbt])\n",
    "\n",
    "pipelineGbtModel = pipelineGbt.fit(dfTrain)\n",
    "\n",
    "dfTestPredictions = pipelineGbtModel.transform(dfTest) \n",
    "\n",
    "dfTestPredictions.show()\n",
    "\n",
    "evaluator.evaluate(dfTestPredictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver claramente la superioridad de este nuevo modelo que hemos entrenado, aunque por otra parte el tiempo de ejecución ha sido más elevado.\n",
    "\n",
    "Muchas veces hemos de sacrificar algo de potencia a cambio de velocidad. Para ello hay que estar atentos al problema que estemos resolviendo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluación de modelos y ajuste de hyperparámetros\n",
    "\n",
    "En el ejercicio anterior hemos contrastado el resultado de dos algoritmos de regresión diferentes y hemos obtenido dos resultados donde claramente uno era superior al anterior. \n",
    "\n",
    "Generalmente se suele evaluar un número de modelos diferente en base a distintas configuraciones, lo ideal sería poder realizar este proceso de manera sistemática y estadísticamente correcta.\n",
    "\n",
    "En spark MLlib tenemos a nuestra disposición uno de los métodos más potentes para poder realizar este proceso, el algoritmo de **GridSearch** ejecutado sobre una Validación Cruzada (**Cross Validation**)\n",
    "\n",
    "Esta funcionalidad está disponible en el paquete `pyspark.ml.tuning`\n",
    "\n",
    "Lo primero que tenemos que hacer es crear una grid de parámetros, es decir, una especificación de distintas combinaciones de hyperparámetros con los que evaluar nuestro modelo. Podemos incluir parámetros tanto para el algoritmo de regresión como para los estimadores que se entrenan para preprocesado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(vectorIndexer.maxCategories, [2, 4, 8]) \\\n",
    "    .addGrid(lr.regParam, [0.1, 0.01]) \\\n",
    "    .build()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora configuraremos la validación cruzada, para lo que necesitamos especificar un estimador (que será nuestra pipeline), un evaluador (RMSE en nuestros caso) y opcionalmente un grid de parámetros. La validación se realizará conforme a un número de folds, que será el número de iteraciones a realizar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crossvalLr = CrossValidator(estimator=pipelineLr,\n",
    "    estimatorParamMaps=paramGrid,\n",
    "    evaluator=RegressionEvaluator(metricName=\"rmse\", labelCol=\"cnt\", predictionCol=\"prediction\") ,\n",
    "    numFolds=5, \n",
    "    seed=1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez definido el proceso podemos entrenarlo con los datos de entrenamiento del mismo modo que hacíamos en el ejemplo anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crossvalModelLr = crossvalLr.fit(dfTrain)\n",
    "\n",
    "print( crossvalModelLr.avgMetrics )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Durante el proceso de entrenamiento se evalúan las distintas folds y se generan modelos intermedios. Podemos comprobar cuál es el resultado de este proceso consultando las métricas así como la media."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(zip(crossvalModelLr.avgMetrics, paramGrid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Además de consultar todas las salidas generadas, el proceso de selección almacena la mejor configuración de todos los modelos. De hecho puede ser utilizado directamente como  transformador como si fuese dicho modelo.\n",
    "\n",
    "El modelo almacenado corresponde con la mejor configuración, **reentrenada para todos los datos de training**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestLrPredictions = crossvalModelLr.transform(dfTest)\n",
    "\n",
    "bestLrPredictions.show()\n",
    "\n",
    "evaluator.evaluate(bestLrPredictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si queremos disponer directamente del modelo en sí podemos recuperarlo, consultar sus parámetros, almacenarlo o utilizarlo más adelante. Es importante recordar que este estimador es el pipeline entero!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestLrModel = crossvalModelLr.bestModel\n",
    "\n",
    "bestPredictionDF = bestLrModel.transform(dfTest)\n",
    "\n",
    "bestPredictionDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cómo seleccionar el mejor modelo\n",
    "\n",
    "Ahora que tenemos preparado todo el proceso de aprendizaje vamos a contrastar de nuevo la mejor regresión contra el mejor ensemble de árboles. Para ello compararemos contra el conjunto de test el mejor modelo de ambos procesos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paramGridGbt = ParamGridBuilder() \\\n",
    "    .addGrid(vectorIndexer.maxCategories, [2, 4]) \\\n",
    "    .addGrid(gbt.maxDepth, [3, 5]) \\\n",
    "    .build()\n",
    "\n",
    "crossvalGBT = CrossValidator(estimator=pipelineGbt,\n",
    "                          estimatorParamMaps=paramGridGbt,\n",
    "                          evaluator=RegressionEvaluator(metricName=\"rmse\", labelCol=\"cnt\", predictionCol=\"prediction\") ,\n",
    "                          numFolds=5, \n",
    "                          seed=1234)\n",
    "\n",
    "crossvalGBTModel = crossvalGBT.fit(dfTrain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestGBTPredictions = crossvalGBTModel.transform(dfTest)\n",
    "\n",
    "evaluator.evaluate(bestGBTPredictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
